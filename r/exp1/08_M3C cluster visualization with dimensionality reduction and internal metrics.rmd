---
title: "08_Cluster visualization with dimensionality reduction"
author: "Andrea Termine"
date: "2022-09-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r config}
# plant a seed
set.seed(12345)

# Path to config
config = yaml::read_yaml(file = "../../config/config.yaml", eval.expr=TRUE)

# Paths
data.path       = file.path(config$exp1$parent, config$exp1$datapath)
rna.meta.path   = file.path(config$exp1$parent, config$exp1$rna.meta.path)
lcf.out         = file.path(config$exp1$parent, config$exp1$lcf.out)
udf.path        = file.path(config$exp1$parent, config$exp1$udf.path)
vst.out         = file.path(config$exp1$parent, config$exp1$vst.out)
vst.ff.out      = file.path(config$exp1$parent, config$exp1$vst.ff.out)
randomdf_path   = file.path(config$exp1$parent, config$clustering$random_df)
ctend.out       = file.path(config$exp1$parent, config$clustering$ctend.out)
```

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
# Import custom functions
source(file.path(udf.path, "functions.r"))

## Required Packages
pkg = c("tidyverse", "yaml", "fst", 
        "hopkins", "FactoMineR", "factoextra", "M3C","Rdimtools","clusterCrit",
        "patchwork", "seriation", "pheatmap", "openxlsx", "ggforce", "concaveman")

## Now load or install & load all // Cran version
load_packages(pkgs = pkg)
# cat("Libraries Imported")
rm(pkg,p)
```

```{r import}
# import metadata ----
rna_seq_metadata = readRDS(file.path(rna.meta.path, config$exp1$metadata_rna))

# storage for every type of counts ----
count.list        = vector(mode = "list", length = length(config$clustering$prep_methods))
names(count.list) = config$clustering$prep_methods

# define an import function ----
count_importer <- function(path, pattern) {
  count.path        = list.files(path = path, pattern = pattern, full.names = T)
  count.list        = vector(mode = "list", length = length(count.path))
  names(count.list) = list.files(path = path, pattern = pattern, full.names = F) %>% str_remove(".fst")
  
  for (i in 1:length(count.path)) {
    count.list[[i]] = read_fst(path = count.path[i])
  }
  
  return(count.list)
}

# set vectors for paths and pattern ----
paths    = c(lcf.out, vst.out, vst.out, vst.ff.out, vst.ff.out)
patterns = c(".fst", "local_counts|mean_counts|parametric_counts", "center",
             "local_ff_counts|mean_ff_counts|parametric_ff_counts", "center")

# Import the counts ----
for (i in 1:length(config$clustering$prep_methods)) {
  count.list[[config$clustering$prep_methods[[i]] ]] = count_importer(path    = paths[i], 
                                                                      pattern = patterns[i])
}

# Import the random df ----
rdf = read.csv(file = file.path(randomdf_path, "random df.csv"))

# Import the winning results from M3C
entropy.solution = readRDS(file = config$clustering$entropy.solution)
pac.solution     = readRDS(file = config$clustering$pac.solution)
```

## Summary

We used M3C to assess cluster tendency and obtain labels. Now we will try to visualize labels using dimensionality reduction methods.The best solution for PD was entropy/km/chol/2. But the pac/km/chol/3 was sligthly better if you consider the rcsi. Anyway, the entropy solution is not based on subjective windows and other stuff reported in the vignette, so we will use it. 

The best dataframes are:
**entropy:pd_lcf_vst_mean_ff_centerscale_counts**
pac:    pd_lcf_vst_local_ff_centerscale_counts

## Entropy solution

We will use Rdimtools to visualize the labels on a 2-d space. First we're going to estimate the dimensions of the dataset. You should have only numerical variables and rows are the observations.Dimension estimation: this is not useful since we don't know the ground truth. Skip it.

```{r}
entropy.counts = count.list$lcf_vst_ff_cs[[config$clustering$entropy.dataset]]

# Create annotation dataframe with label and remove IDs from the counts
entropy.demo   = entropy.counts %>% dplyr::select(PATNO) %>% mutate(label = entropy.solution$assignments)
entropy.counts = entropy.counts %>% dplyr::select(-PATNO)

# Estimate dimension for the dataset
# we will compare 6 methods (out of 17 methods from version 1.0.0)
vecd = rep(0,6)
vecd[1] = est.Ustat(entropy.counts)$estdim               # convergence rate of U-statistic on manifold
vecd[2] = est.correlation(entropy.counts)$estdim         # correlation dimension
vecd[3] = est.made(entropy.counts)$estdim                # manifold-adaptive dimension estimation
vecd[4] = est.mle2(entropy.counts)$estdim                # MLE with Poisson process and bias correction
vecd[5] = est.twonn(entropy.counts)$estdim               # minimal neighborhood information
vecd[6] = est.packing(entropy.counts, eps = 0.01)$estdim # Intrinsic Dimension Estimation using Packing Numbers

# Perform the classical pca
entropy.pca = PCA(entropy.counts, scale.unit = FALSE, graph = FALSE)

fviz_pca_ind(entropy.pca,
             habillage = entropy.demo$label %>% as.character() %>% as.factor(),
             label    = "none")

M3C::tsne(t(entropy.counts), labels = entropy.demo$label %>% as.character())

M3C::umap(t(entropy.counts), labels = entropy.demo$label %>% as.character())
```

## Silhouette

Here we use the intCriteria function. It should compute automatically the dist matrix.

```{r}
intCriteria = intCriteria(traj = entropy.counts %>% as.matrix,
                          part = entropy.solution$assignments %>% as.integer(),
                          crit = c("all"))

data.frame(clustering_method  = "M3C",
           method             = names(intCriteria),
           value              = intCriteria %>% paste(sep = "") %>% as.numeric()) %>% 
  write.xlsx(config$clustering$m3c.internal.metrics)


```

## Label export

```{r}
write.xlsx(entropy.demo, config$clustering$m3c.cluster.labels)
```


## Sex and subgroup check

Should we inspect the effect size? or the discriminative power of these two variables?

```{r}
check = left_join(entropy.demo, 
          rna_seq_metadata %>% 
            filter(CLINICAL_EVENT == "BL") %>% 
            dplyr::select(PATNO, GENDER, Cohort, Subgroup) %>% 
            mutate(PATNO =as.character(PATNO)),
          by = "PATNO")

xtabs(~ GENDER + Cohort,   data = check)
xtabs(~ GENDER + Subgroup, data = check)

xtabs(~ label + GENDER, data = check) %>% chisq.test()
xtabs(~ GENDER + label, data = check) %>% prop.table(margin = 2) %>% `*`(100) %>% addmargins()

fviz_pca_ind(entropy.pca,
             habillage = check$GENDER %>% as.factor(),
             label    = "none") +
  geom_mark_hull(aes(group  = entropy.demo$label %>% as.character() %>% as.factor(),
                     fill   = entropy.demo$label %>% as.character() %>% as.factor(),
                     label  = entropy.demo$label %>% as.character() %>% as.factor()),
                 concavity      = 5,
                 expand         = unit(2, "mm"),
                 alpha          = 0.2,
                 label.fontsize = 8,
                 label.buffer   = unit(0.6, 'mm'),
                 show.legend    = FALSE) 

fviz_pca_ind(entropy.pca,
             habillage = check$Subgroup %>% as.factor(),
             label    = "none") +
  geom_mark_hull(aes(group  = entropy.demo$label %>% as.character() %>% as.factor(),
                     fill   = entropy.demo$label %>% as.character() %>% as.factor(),
                     label  = entropy.demo$label %>% as.character() %>% as.factor()),
                 concavity      = 5,
                 expand         = unit(2, "mm"),
                 alpha          = 0.2,
                 label.fontsize = 8,
                 label.buffer   = unit(0.6, 'mm'),
                 show.legend    = FALSE) 

xtabs(~ Subgroup + label, data = check) %>% chisq.test()
xtabs(~ Subgroup + label, data = check) %>% prop.table(margin = 2) %>% `*`(100) %>% addmargins()
```

